{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab35dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages in Colab\n",
    "!pip install librosa scikit-learn numpy pandas matplotlib seaborn soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77766e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import os\n",
    "from google.colab import files\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5136ac",
   "metadata": {},
   "source": [
    "## Step 1: Define Feature Extraction (Must Match Backend)\n",
    "\n",
    "This function must extract features EXACTLY as the backend does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42ad78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_voice_features(audio_path):\n",
    "    \"\"\"\n",
    "    Extract voice features using librosa - MUST MATCH backend preprocessing\n",
    "    Returns feature vector matching backend's preprocess_audio function\n",
    "    \"\"\"\n",
    "    # Load audio with librosa\n",
    "    y, sr = librosa.load(audio_path, sr=22050)\n",
    "    \n",
    "    # Extract MFCC features (standard for voice analysis)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    mfccs_mean = np.mean(mfccs, axis=1)\n",
    "    mfccs_std = np.std(mfccs, axis=1)\n",
    "    \n",
    "    # Extract spectral features\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "    spectral_centroid_mean = np.mean(spectral_centroid)\n",
    "    spectral_centroid_std = np.std(spectral_centroid)\n",
    "    \n",
    "    # Extract zero crossing rate\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "    zcr_mean = np.mean(zcr)\n",
    "    zcr_std = np.std(zcr)\n",
    "    \n",
    "    # Extract chroma features\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    chroma_mean = np.mean(chroma, axis=1)\n",
    "    chroma_std = np.std(chroma, axis=1)\n",
    "    \n",
    "    # Extract RMS energy\n",
    "    rms = librosa.feature.rms(y=y)\n",
    "    rms_mean = np.mean(rms)\n",
    "    rms_std = np.std(rms)\n",
    "    \n",
    "    # Combine all features into a single vector (MUST MATCH BACKEND)\n",
    "    features = np.concatenate([\n",
    "        mfccs_mean,  # 13 features\n",
    "        mfccs_std,   # 13 features\n",
    "        [spectral_centroid_mean, spectral_centroid_std],  # 2 features\n",
    "        [zcr_mean, zcr_std],  # 2 features\n",
    "        chroma_mean,  # 12 features\n",
    "        chroma_std,   # 12 features\n",
    "        [rms_mean, rms_std]  # 2 features\n",
    "    ])\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adadce26",
   "metadata": {},
   "source": [
    "## Step 2: Load Your Audio Dataset\n",
    "\n",
    "**Option A: Upload your own labeled audio files**\n",
    "- Create folders: `stressed_audio/` and `calm_audio/`\n",
    "- Put audio files (.wav, .mp3) in respective folders\n",
    "- Upload to Colab\n",
    "\n",
    "**Option B: Use a public dataset (e.g., RAVDESS)**\n",
    "- Map emotions to stress/calm\n",
    "- Angry, Fearful, Sad ‚Üí Stressed (label 1)\n",
    "- Happy, Calm, Neutral ‚Üí Not Stressed (label 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d248f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_dataset(stressed_folder, calm_folder):\n",
    "    \"\"\"\n",
    "    Load audio files from folders and extract features\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # Load stressed audio (label 1)\n",
    "    print(\"Loading stressed audio files...\")\n",
    "    if os.path.exists(stressed_folder):\n",
    "        for filename in os.listdir(stressed_folder):\n",
    "            if filename.endswith(('.wav', '.mp3', '.webm', '.ogg')):\n",
    "                try:\n",
    "                    audio_path = os.path.join(stressed_folder, filename)\n",
    "                    features = extract_voice_features(audio_path)\n",
    "                    X.append(features)\n",
    "                    y.append(1)  # Stressed\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filename}: {e}\")\n",
    "    \n",
    "    # Load calm audio (label 0)\n",
    "    print(\"Loading calm audio files...\")\n",
    "    if os.path.exists(calm_folder):\n",
    "        for filename in os.listdir(calm_folder):\n",
    "            if filename.endswith(('.wav', '.mp3', '.webm', '.ogg')):\n",
    "                try:\n",
    "                    audio_path = os.path.join(calm_folder, filename)\n",
    "                    features = extract_voice_features(audio_path)\n",
    "                    X.append(features)\n",
    "                    y.append(0)  # Not stressed\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filename}: {e}\")\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load your dataset\n",
    "# X, y = load_audio_dataset('path/to/stressed_audio', 'path/to/calm_audio')\n",
    "\n",
    "# OR for quick testing, create synthetic data (REPLACE WITH REAL DATA FOR PRODUCTION)\n",
    "print(\"Creating synthetic training data for demonstration...\")\n",
    "print(\"‚ö†Ô∏è REPLACE THIS WITH REAL LABELED AUDIO FILES FOR PRODUCTION!\")\n",
    "n_samples = 500\n",
    "n_features = 56  # Must match feature vector size (13+13+2+2+12+12+2)\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "y = np.random.randint(0, 2, n_samples)\n",
    "\n",
    "print(f\"Dataset loaded: {len(X)} samples, {X.shape[1]} features\")\n",
    "print(f\"Class distribution: Stressed={np.sum(y==1)}, Calm={np.sum(y==0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d651f9",
   "metadata": {},
   "source": [
    "## Step 3: Visualize Features (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c48fdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(X[y==0, 0], alpha=0.5, label='Calm', bins=20)\n",
    "plt.hist(X[y==1, 0], alpha=0.5, label='Stressed', bins=20)\n",
    "plt.xlabel('MFCC 1 Mean')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.title('Feature Distribution: MFCC 1')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(X[y==0, 26], alpha=0.5, label='Calm', bins=20)\n",
    "plt.hist(X[y==1, 26], alpha=0.5, label='Stressed', bins=20)\n",
    "plt.xlabel('Spectral Centroid Mean')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.title('Feature Distribution: Spectral Centroid')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(X[y==0, 28], alpha=0.5, label='Calm', bins=20)\n",
    "plt.hist(X[y==1, 28], alpha=0.5, label='Stressed', bins=20)\n",
    "plt.xlabel('Zero Crossing Rate Mean')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.title('Feature Distribution: ZCR')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40edc2da",
   "metadata": {},
   "source": [
    "## Step 4: Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c1618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2b1753",
   "metadata": {},
   "source": [
    "## Step 5: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b0bfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest Classifier\n",
    "print(\"Training Random Forest model...\")\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=150,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "print(\"‚úì Model training complete!\")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "print(f\"\\nCross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean CV accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba83565",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4726cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_proba = model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Calm', 'Stressed']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', \n",
    "            xticklabels=['Calm', 'Stressed'],\n",
    "            yticklabels=['Calm', 'Stressed'])\n",
    "plt.title('Confusion Matrix - Voice Stress Detection')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance\n",
    "feature_names = [\n",
    "    'MFCC_mean_' + str(i) for i in range(13)\n",
    "] + [\n",
    "    'MFCC_std_' + str(i) for i in range(13)\n",
    "] + [\n",
    "    'SpectralCentroid_mean', 'SpectralCentroid_std',\n",
    "    'ZCR_mean', 'ZCR_std'\n",
    "] + [\n",
    "    'Chroma_mean_' + str(i) for i in range(12)\n",
    "] + [\n",
    "    'Chroma_std_' + str(i) for i in range(12)\n",
    "] + [\n",
    "    'RMS_mean', 'RMS_std'\n",
    "]\n",
    "\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1][:15]  # Top 15 features\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title('Top 15 Most Important Features')\n",
    "plt.bar(range(15), importances[indices])\n",
    "plt.xticks(range(15), [feature_names[i] for i in indices], rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e3df36",
   "metadata": {},
   "source": [
    "## Step 7: Export Model as .pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924796c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that includes the scaler and model\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create pipeline\n",
    "voice_pipeline = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "# Save model\n",
    "model_filename = 'voice_stress_model.pkl'\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(voice_pipeline, f)\n",
    "\n",
    "print(f\"‚úì Model saved as {model_filename}\")\n",
    "print(f\"Model size: {os.path.getsize(model_filename) / 1024:.2f} KB\")\n",
    "\n",
    "# Download the model\n",
    "print(\"\\nDownloading model...\")\n",
    "files.download(model_filename)\n",
    "print(\"‚úì Download complete! Place this file in backend/models/ folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1a660d",
   "metadata": {},
   "source": [
    "## Step 8: Test the Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf021f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "with open(model_filename, 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "# Test with a sample\n",
    "sample = X_test[0:1]\n",
    "prediction = loaded_model.predict(sample)\n",
    "probability = loaded_model.predict_proba(sample)\n",
    "\n",
    "print(\"Test prediction:\")\n",
    "print(f\"Predicted class: {prediction[0]} ({'Stressed' if prediction[0] == 1 else 'Calm'})\")\n",
    "print(f\"Probabilities: Calm={probability[0][0]:.3f}, Stressed={probability[0][1]:.3f}\")\n",
    "print(f\"Actual class: {y_test[0]} ({'Stressed' if y_test[0] == 1 else 'Calm'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee736fe7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Voice Model Training Complete!\n",
    "\n",
    "**Next Steps:**\n",
    "1. Download the `voice_stress_model.pkl` file\n",
    "2. Place it in your project's `backend/models/` folder\n",
    "3. Ensure you also have the face model trained\n",
    "4. Start the backend server and test the complete pipeline!\n",
    "\n",
    "**Tips for Better Voice Models:**\n",
    "- Use a diverse audio dataset (different speakers, ages, languages)\n",
    "- Balance classes (equal stressed/calm samples)\n",
    "- Consider pitch, tempo, and prosody features\n",
    "- Try SVM with RBF kernel for potentially better results\n",
    "- Add data augmentation (time stretching, pitch shifting)\n",
    "- Experiment with deep learning (CNN, LSTM) for even better accuracy\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
