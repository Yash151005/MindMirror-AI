{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6b1c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages in Colab\n",
    "!pip install opencv-python scikit-learn numpy pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036f3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import os\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2fc0b5",
   "metadata": {},
   "source": [
    "## Step 1: Define Feature Extraction (Must Match Backend)\n",
    "\n",
    "This function must extract features EXACTLY as the backend does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1dce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face_features(image_path):\n",
    "    \"\"\"\n",
    "    Extract face features using OpenCV - MUST MATCH backend preprocessing\n",
    "    Returns feature vector matching backend's preprocess_image function\n",
    "    \"\"\"\n",
    "    # Read image\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Resize to standard size (48x48)\n",
    "    resized = cv2.resize(gray, (48, 48))\n",
    "    \n",
    "    # Normalize pixel values\n",
    "    normalized = resized / 255.0\n",
    "    \n",
    "    # Extract histogram features\n",
    "    hist = cv2.calcHist([resized], [0], None, [256], [0, 256])\n",
    "    hist = hist.flatten() / hist.sum()  # Normalize histogram\n",
    "    \n",
    "    # Calculate statistical features\n",
    "    mean_val = np.mean(normalized)\n",
    "    std_val = np.std(normalized)\n",
    "    \n",
    "    # Edge detection features\n",
    "    edges = cv2.Canny(resized, 50, 150)\n",
    "    edge_density = np.sum(edges > 0) / edges.size\n",
    "    \n",
    "    # Combine all features into a single vector (MUST MATCH BACKEND)\n",
    "    features = np.concatenate([\n",
    "        normalized.flatten(),  # Pixel values (2304 features)\n",
    "        hist[:64],  # Reduced histogram (64 bins)\n",
    "        [mean_val, std_val, edge_density]  # Statistical features (3 features)\n",
    "    ])\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c4c925",
   "metadata": {},
   "source": [
    "## Step 2: Load Your Dataset\n",
    "\n",
    "**Option A: Upload your own labeled images**\n",
    "- Create folders: `stressed/` and `not_stressed/`\n",
    "- Put face images in respective folders\n",
    "- Upload to Colab\n",
    "\n",
    "**Option B: Use a public dataset (e.g., FER-2013)**\n",
    "- Map emotions to stress/no-stress\n",
    "- Angry, Sad, Fear ‚Üí Stressed (label 1)\n",
    "- Happy, Neutral, Surprise ‚Üí Not Stressed (label 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bdef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load images from folders\n",
    "# Adjust paths based on your dataset structure\n",
    "\n",
    "def load_dataset_from_folders(stressed_folder, not_stressed_folder):\n",
    "    \"\"\"\n",
    "    Load images from folders and extract features\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    # Load stressed images (label 1)\n",
    "    print(\"Loading stressed images...\")\n",
    "    for filename in os.listdir(stressed_folder):\n",
    "        if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            try:\n",
    "                image_path = os.path.join(stressed_folder, filename)\n",
    "                features = extract_face_features(image_path)\n",
    "                X.append(features)\n",
    "                y.append(1)  # Stressed\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "    \n",
    "    # Load not stressed images (label 0)\n",
    "    print(\"Loading not stressed images...\")\n",
    "    for filename in os.listdir(not_stressed_folder):\n",
    "        if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            try:\n",
    "                image_path = os.path.join(not_stressed_folder, filename)\n",
    "                features = extract_face_features(image_path)\n",
    "                X.append(features)\n",
    "                y.append(0)  # Not stressed\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load your dataset\n",
    "# X, y = load_dataset_from_folders('path/to/stressed', 'path/to/not_stressed')\n",
    "\n",
    "# OR for quick testing, create synthetic data (REPLACE WITH REAL DATA FOR PRODUCTION)\n",
    "print(\"Creating synthetic training data for demonstration...\")\n",
    "print(\"‚ö†Ô∏è REPLACE THIS WITH REAL LABELED FACE IMAGES FOR PRODUCTION!\")\n",
    "n_samples = 500\n",
    "n_features = 2371  # Must match feature vector size\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "y = np.random.randint(0, 2, n_samples)\n",
    "\n",
    "print(f\"Dataset loaded: {len(X)} samples, {X.shape[1]} features\")\n",
    "print(f\"Class distribution: Stressed={np.sum(y==1)}, Not Stressed={np.sum(y==0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a751391",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c882c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd84095",
   "metadata": {},
   "source": [
    "## Step 4: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7215bd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest Classifier\n",
    "print(\"Training Random Forest model...\")\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "print(\"‚úì Model training complete!\")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n",
    "print(f\"\\nCross-validation scores: {cv_scores}\")\n",
    "print(f\"Mean CV accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std():.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db908618",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe01d841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_proba = model.predict_proba(X_test_scaled)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Not Stressed', 'Stressed']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Not Stressed', 'Stressed'],\n",
    "            yticklabels=['Not Stressed', 'Stressed'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b290a9",
   "metadata": {},
   "source": [
    "## Step 6: Export Model as .pkl\n",
    "\n",
    "This creates a pickle file that you can download and use in your backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433e8fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that includes the scaler and model\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create pipeline\n",
    "face_pipeline = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('classifier', model)\n",
    "])\n",
    "\n",
    "# Save model\n",
    "model_filename = 'face_stress_model.pkl'\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(face_pipeline, f)\n",
    "\n",
    "print(f\"‚úì Model saved as {model_filename}\")\n",
    "print(f\"Model size: {os.path.getsize(model_filename) / 1024:.2f} KB\")\n",
    "\n",
    "# Download the model\n",
    "print(\"\\nDownloading model...\")\n",
    "files.download(model_filename)\n",
    "print(\"‚úì Download complete! Place this file in backend/models/ folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318819eb",
   "metadata": {},
   "source": [
    "## Step 7: Test the Model (Optional)\n",
    "\n",
    "Verify the model works correctly before deploying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08f9275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "with open(model_filename, 'rb') as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "# Test with a sample\n",
    "sample = X_test[0:1]\n",
    "prediction = loaded_model.predict(sample)\n",
    "probability = loaded_model.predict_proba(sample)\n",
    "\n",
    "print(\"Test prediction:\")\n",
    "print(f\"Predicted class: {prediction[0]} ({'Stressed' if prediction[0] == 1 else 'Not Stressed'})\")\n",
    "print(f\"Probabilities: Not Stressed={probability[0][0]:.3f}, Stressed={probability[0][1]:.3f}\")\n",
    "print(f\"Actual class: {y_test[0]} ({'Stressed' if y_test[0] == 1 else 'Not Stressed'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2ff1c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Model Training Complete!\n",
    "\n",
    "**Next Steps:**\n",
    "1. Download the `face_stress_model.pkl` file\n",
    "2. Place it in your project's `backend/models/` folder\n",
    "3. The backend will automatically load and use this model\n",
    "4. Train the voice model using the companion notebook\n",
    "\n",
    "**Tips for Better Models:**\n",
    "- Use a larger, diverse dataset (1000+ images)\n",
    "- Balance classes (equal stressed/not stressed samples)\n",
    "- Try different classifiers (SVM, XGBoost, Neural Networks)\n",
    "- Tune hyperparameters using GridSearchCV\n",
    "- Add data augmentation for more training samples\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
